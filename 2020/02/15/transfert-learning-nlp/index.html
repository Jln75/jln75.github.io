<!DOCTYPE html><html lang="en" data-theme="dark"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5"><title>Transfert Learning for NLP | Julien Zouein</title><meta name="description" content="Transfert Learning has greatly improved NLP tasks*. *By giving access to pre-trained model, it is now easier to train Natural Language Processing model, it is faster, better and requires less data since we are not training from scratch anymore. From ULMFit to RoBERTa it is interesting to see what are the improvement proposed to make transfert learning."><meta name="keywords" content="NLP,Deep Learning,short paper"><meta name="author" content="Julien Zouein"><meta name="copyright" content="Julien Zouein"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon.ico"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="https://fonts.googleapis.com" crossorigin><link rel="preconnect" href="//busuanzi.ibruce.info"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Transfert Learning for NLP"><meta name="twitter:description" content="Transfert Learning has greatly improved NLP tasks*. *By giving access to pre-trained model, it is now easier to train Natural Language Processing model, it is faster, better and requires less data since we are not training from scratch anymore. From ULMFit to RoBERTa it is interesting to see what are the improvement proposed to make transfert learning."><meta name="twitter:image" content="http://yoursite.com/img/cover/bert.png"><meta property="og:type" content="article"><meta property="og:title" content="Transfert Learning for NLP"><meta property="og:url" content="http://yoursite.com/2020/02/15/transfert-learning-nlp/"><meta property="og:site_name" content="Julien Zouein"><meta property="og:description" content="Transfert Learning has greatly improved NLP tasks*. *By giving access to pre-trained model, it is now easier to train Natural Language Processing model, it is faster, better and requires less data since we are not training from scratch anymore. From ULMFit to RoBERTa it is interesting to see what are the improvement proposed to make transfert learning."><meta property="og:image" content="http://yoursite.com/img/cover/bert.png"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script src="https://cdn.jsdelivr.net/npm/js-cookie/dist/js.cookie.min.js"></script><script>const autoChangeMode = 'false'
var t = Cookies.get("theme");
if (autoChangeMode == '1'){
const isDarkMode = window.matchMedia("(prefers-color-scheme: dark)").matches
const isLightMode = window.matchMedia("(prefers-color-scheme: light)").matches
const isNotSpecified = window.matchMedia("(prefers-color-scheme: no-preference)").matches
const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

if (t === undefined){
  if (isLightMode) activateLightMode()
  else if (isDarkMode) activateDarkMode()
  else if (isNotSpecified || hasNoSupport){
    console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
    now = new Date();
    hour = now.getHours();
    isNight = hour < 6 || hour >= 18
    isNight ? activateDarkMode() : activateLightMode()
}
} else if (t == 'light') activateLightMode()
else activateDarkMode()


} else if (autoChangeMode == '2'){
  now = new Date();
  hour = now.getHours();
  isNight = hour < 6 || hour >= 18
  if(t === undefined) isNight? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode() 
} else {
  if ( t == 'dark' ) activateDarkMode()
  else if ( t == 'light') activateLightMode()
}

function activateDarkMode(){
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null){
    document.querySelector('meta[name="theme-color"]').setAttribute('content','#000')
  }
}
function activateLightMode(){
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null){
  document.querySelector('meta[name="theme-color"]').setAttribute('content','#fff')
  }
}</script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="canonical" href="http://yoursite.com/2020/02/15/transfert-learning-nlp/"><link rel="prev" title="Machine Learning Introduction" href="http://yoursite.com/2020/03/30/machine-learning-intro/"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  bookmark: {
    title: 'Snackbar.bookmark.title',
    message_prev: 'Press',
    message_next: 'to bookmark this page'
  },
  runtime_unit: 'days',
  runtime: false,
  copyright: undefined,
  ClickShowText: undefined,
  medium_zoom: false,
  fancybox: true,
  Snackbar: undefined,
  baiduPush: false,
  isHome: false,
  isPost: true
  
}</script><meta name="generator" content="Hexo 4.2.0"></head><body><header> <div id="page-header"><span class="pull_left" id="blog_name"><a class="blog_title" id="site-name" href="/">Julien Zouein</a></span><span class="toggle-menu pull_right close"><a class="site-page"><i class="fa fa-bars fa-fw" aria-hidden="true"></i></a></span><span class="pull_right menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> Post</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> About</span></a></div></div></span></div></header><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="/img/profile.jpg" onerror="onerror=null;src='/img/wallpaper_default.jpg'" alt="avatar"></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">Articles</div><div class="length_num">6</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">Tags</div><div class="length_num">16</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">Categories</div><div class="length_num">10</div></a></div></div></div><hr><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> Post</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> About</span></a></div></div></div><div id="mobile-sidebar-toc"><div class="toc_mobile_headline">Catalog</div><div class="sidebar-toc__content"><ol class="toc_mobile_items"><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#Transfert-Learning-in-NLP"><span class="toc_mobile_items-number">1.</span> <span class="toc_mobile_items-text">Transfert Learning in NLP</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#Abstract"><span class="toc_mobile_items-number">1.1.</span> <span class="toc_mobile_items-text">Abstract</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#Introduction"><span class="toc_mobile_items-number">1.2.</span> <span class="toc_mobile_items-text">Introduction</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#Related-Work"><span class="toc_mobile_items-number">1.3.</span> <span class="toc_mobile_items-text">Related Work</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#How-Transferable-are-Neural-Networks-in-NLP-Applications"><span class="toc_mobile_items-number">1.3.1.</span> <span class="toc_mobile_items-text">How Transferable are Neural Networks in NLP Applications?</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#Method-1"><span class="toc_mobile_items-number">1.3.1.1.</span> <span class="toc_mobile_items-text">Method 1</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#Method-2"><span class="toc_mobile_items-number">1.3.1.2.</span> <span class="toc_mobile_items-text">Method 2</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#Parameter-Efficient-Transfer-Learning-for-NLP"><span class="toc_mobile_items-number">1.3.2.</span> <span class="toc_mobile_items-text">Parameter-Efficient Transfer Learning for NLP</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#Transfer-Learning-in-Biomedical-Natural-Language-Processing-An-Evaluation-of-BERT-and-ELMo-on-Ten-Benchmarking-Datasets"><span class="toc_mobile_items-number">1.3.3.</span> <span class="toc_mobile_items-text">Transfer Learning in Biomedical Natural Language Processing: An Evaluation of BERT and ELMo on Ten Benchmarking Datasets</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#Sentence-Similarity"><span class="toc_mobile_items-number">1.3.3.1.</span> <span class="toc_mobile_items-text">Sentence Similarity</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#Named-Entity-Recognition"><span class="toc_mobile_items-number">1.3.3.2.</span> <span class="toc_mobile_items-text">Named Entity Recognition</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#Relation-Extraction"><span class="toc_mobile_items-number">1.3.3.3.</span> <span class="toc_mobile_items-text">Relation Extraction</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#Document-multilabel-classification"><span class="toc_mobile_items-number">1.3.3.4.</span> <span class="toc_mobile_items-text">Document multilabel classification</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#Inference-Task"><span class="toc_mobile_items-number">1.3.3.5.</span> <span class="toc_mobile_items-text">Inference Task</span></a></li></ol></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#Transfert-Learning-from-supervised-Data"><span class="toc_mobile_items-number">1.4.</span> <span class="toc_mobile_items-text">Transfert Learning from supervised Data</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#Architecture"><span class="toc_mobile_items-number">1.4.1.</span> <span class="toc_mobile_items-text">Architecture</span></a></li></ol></li></ol></li></ol></div></div></div><div id="body-wrap"><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true">     </i><div id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Transfert-Learning-in-NLP"><span class="toc-number">1.</span> <span class="toc-text">Transfert Learning in NLP</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Abstract"><span class="toc-number">1.1.</span> <span class="toc-text">Abstract</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Introduction"><span class="toc-number">1.2.</span> <span class="toc-text">Introduction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Related-Work"><span class="toc-number">1.3.</span> <span class="toc-text">Related Work</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#How-Transferable-are-Neural-Networks-in-NLP-Applications"><span class="toc-number">1.3.1.</span> <span class="toc-text">How Transferable are Neural Networks in NLP Applications?</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Method-1"><span class="toc-number">1.3.1.1.</span> <span class="toc-text">Method 1</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Method-2"><span class="toc-number">1.3.1.2.</span> <span class="toc-text">Method 2</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Parameter-Efficient-Transfer-Learning-for-NLP"><span class="toc-number">1.3.2.</span> <span class="toc-text">Parameter-Efficient Transfer Learning for NLP</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Transfer-Learning-in-Biomedical-Natural-Language-Processing-An-Evaluation-of-BERT-and-ELMo-on-Ten-Benchmarking-Datasets"><span class="toc-number">1.3.3.</span> <span class="toc-text">Transfer Learning in Biomedical Natural Language Processing: An Evaluation of BERT and ELMo on Ten Benchmarking Datasets</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Sentence-Similarity"><span class="toc-number">1.3.3.1.</span> <span class="toc-text">Sentence Similarity</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Named-Entity-Recognition"><span class="toc-number">1.3.3.2.</span> <span class="toc-text">Named Entity Recognition</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Relation-Extraction"><span class="toc-number">1.3.3.3.</span> <span class="toc-text">Relation Extraction</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Document-multilabel-classification"><span class="toc-number">1.3.3.4.</span> <span class="toc-text">Document multilabel classification</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Inference-Task"><span class="toc-number">1.3.3.5.</span> <span class="toc-text">Inference Task</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Transfert-Learning-from-supervised-Data"><span class="toc-number">1.4.</span> <span class="toc-text">Transfert Learning from supervised Data</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Architecture"><span class="toc-number">1.4.1.</span> <span class="toc-text">Architecture</span></a></li></ol></li></ol></li></ol></div></div></div><main id="content-outer"><div id="top-container" style="background-image: url(/img/cover/bert.png)"><div id="post-info"><div id="post-title"><div class="posttitle">Transfert Learning for NLP</div></div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> Created 2020-02-15<span class="post-meta__separator">|</span><i class="fa fa-history fa-fw" aria-hidden="true"></i> Updated 2020-03-01</time><span class="post-meta__separator">|</span><span><i class="fa fa-inbox post-meta__icon fa-fw" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/Short-paper/">Short paper</a><i class="fa fa-angle-right fa-fw" aria-hidden="true"></i><i class="fa fa-inbox post-meta__icon fa-fw" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/Short-paper/NLP/">NLP</a></span><div class="post-meta-wordcount"><div class="post-meta-pv-cv"><span><i class="fa fa-eye post-meta__icon fa-fw" aria-hidden="true"> </i>Post View:</span><span id="busuanzi_value_page_pv"></span></div></div></div></div></div><div class="layout layout_post" id="content-inner">   <article id="post"><div class="article-container" id="post-content"><h1 id="Transfert-Learning-in-NLP"><a href="#Transfert-Learning-in-NLP" class="headerlink" title="Transfert Learning in NLP"></a>Transfert Learning in NLP</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a><em>Abstract</em></h2><p><em>Transfert Learning has greatly improved NLP tasks</em>. <em>By giving access to pre-trained model</em>, <em>it is now easier to train Natural Language Processing model</em>, <em>it is faster, better and requires less data since we are not training from scratch anymore</em>.<br><em>From ULMFit to RoBERTa it is interesting to see what are the improvement proposed to make transfert learning</em>.</p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>Inductive transfert learning has had a large impact on Computer Vision (CV). Applied CV models are rarely trained from scratch, but are fine-tuned from general models that have been previously trained on a big image dataset, ImageNet, COCO dataset or others. Unfortunately, those method didn’t exists for Natural Lan- guage Processing (NLP), causing NLP models to take a lot of time and data to be trained and have a good result. Recently new solutions appears, in this paper we will present the latest news concerning transfert learning in NLP. From paper published in 2018 to latest implementa- tion of RoBERTa in Facebook AI laboratory, the idea is to present a general overview of the existing solutions to perform transfert learning in NLP.</p>
<p>We selected three paper to perform this state of the art :</p>
<ul>
<li>Universal Language Model Fine-tuning for Text Clas- sification proposed by Jeremy Howard from Fast.ai and Sebastian Ruder from DeepMind</li>
<li>Pre-training of Deep Bidirectional Transformers for Language Understanding proposed by Jacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova from Google AI Language Team</li>
<li>A Robustly Optimized BERT Pretraining Approach proposed by Yinhan Liu Myle Ott Naman Goyal Jingfei Du Mandar Joshi Danqi Chen Omer Levy Mike Lewis Luke Zettlemoyer Veselin Stoyanov from Facebook AI Laboratory.</li>
</ul>
<p>Our goal is to present and understand the method used to perform transfert learning in NLP field. We will also try to propose and develop some hints for further optimisations.</p>
<h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><p>We decided to discuss three papers, however a lot more paper are talking about this topic. We will present quickly some of them in order to give the opportunity to the reader to improve his understanding of the field.</p>
<h3 id="How-Transferable-are-Neural-Networks-in-NLP-Applications"><a href="#How-Transferable-are-Neural-Networks-in-NLP-Applications" class="headerlink" title="How Transferable are Neural Networks in NLP Applications?"></a>How Transferable are Neural Networks in NLP Applications?</h3><p>This paper is proposed by Lili Mou, Zhao Meng, Rui Yan, Ge Li, Yan Xu, Lu Zhang, Zhi Jin from Pekin university. They treated the question ”How transferable are Neural Networks in NLP Application?”.<br>They distinguished two scenarios :</p>
<ol>
<li>transferring knowledge to a semantically similar/equivalent task but with a different dataset.</li>
<li>transferring knowledge to a task that is semantically different but shares the same neural topoogy / architecture so that neural parameters can indeed be transferred.</li>
</ol>
<p>They focused on two transfert method</p>
<h4 id="Method-1"><a href="#Method-1" class="headerlink" title="Method 1"></a>Method 1</h4><p>Using parameters trained from $\mathcal{S}$ to initialize $\mathcal{T}$.</p>
<h4 id="Method-2"><a href="#Method-2" class="headerlink" title="Method 2"></a>Method 2</h4><p>Multi-task learning, which means training S and T simultaneously.</p>
<p>the paper is focused on three principal questions which are</p>
<ol>
<li>How transferable are neural networks between two tasks with similar or different semantics in NLP applications?</li>
<li>How transferable are different layers of NLP neural models?</li>
<li>How transferable are INIT and MULT, respectively? What is the effect of combining these two methods?</li>
</ol>
<h3 id="Parameter-Efficient-Transfer-Learning-for-NLP"><a href="#Parameter-Efficient-Transfer-Learning-for-NLP" class="headerlink" title="Parameter-Efficient Transfer Learning for NLP"></a>Parameter-Efficient Transfer Learning for NLP</h3><p>This paper written by Neil Houlsby Andrei Giurgiu Sta- nisl􏰀aw Jastrzebski Bruna Morrone Quentin de Laroussilhe Andrea Gesmundo Mona Attariyan, Sylvain Gelly.<br>The goal of this paper is to present a new set of parameters to be used to improve transfert learning.</p>
<p>In this paper they proposed new way to improve transfert learning, by introducing adapter modules, who can be used to optimize transfert. Their strategy has three key properties</p>
<ol>
<li>It attains good performances</li>
<li>it permits training on tasks sequentially, that is, it does not require simultaneous access to all datasets</li>
<li>it adds only a small number of additional parameters per task</li>
</ol>
<p>These properties are especially useful in the context of cloud services, where many models need to be trained on a series of downstream tasks, so a high degree of sharing is desirable.</p>
<p>To achieve those properties they proposed a new bot- tleneck adapter module. Tuning with adapter modules involves adding a small number of new parameters to a model, which are trained on the downstream task.</p>
<p>When you use ”classical” fine-tuning element, you have to modify the top layer of the network to adjust to the current task you want to treat. They decided to create a solution used to perform more general architecture modifications. This solution is called Adapter modules. It is used to re-purpose a pre-trained network for a downstream task. Adapter modules have the particularity to inject new layer in the basic network to adapt to the task.</p>
<p>The weight of the original network are untouched while the parameters of added layer are randomly initialized. In that way, the original network can be shared between a lot of application and the Adapters modules are here to adapt the network to the given task.</p>
<h3 id="Transfer-Learning-in-Biomedical-Natural-Language-Processing-An-Evaluation-of-BERT-and-ELMo-on-Ten-Benchmarking-Datasets"><a href="#Transfer-Learning-in-Biomedical-Natural-Language-Processing-An-Evaluation-of-BERT-and-ELMo-on-Ten-Benchmarking-Datasets" class="headerlink" title="Transfer Learning in Biomedical Natural Language Processing: An Evaluation of BERT and ELMo on Ten Benchmarking Datasets"></a>Transfer Learning in Biomedical Natural Language Processing: An Evaluation of BERT and ELMo on Ten Benchmarking Datasets</h3><p>This paper is written by Yifan Peng Shankai Yan Zhiy- ong Lu from National Center for Biotechnology Informa- tion National Library of Medicine, National Institutes of Health Bethesda, MD, USA.</p>
<p>This paper is not on a solution to realize transfert learning, but it introduce a new way to realize a benchmark of transfert learning solution.</p>
<p>Having a good benchmark solution is helping research. By making a benchmark for given task, you have the possibility to understand more precisely how your solu- tion is performing on real task. This benchmark is in the biomedical field.<br>The given solution in the paper follows five tasks</p>
<h4 id="Sentence-Similarity"><a href="#Sentence-Similarity" class="headerlink" title="Sentence Similarity"></a>Sentence Similarity</h4><p>We use several corpus of sen- tences written in pairs and annotated by several expert. The experts give a similarity grade, the idea is to see if the model can find the same grade.</p>
<h4 id="Named-Entity-Recognition"><a href="#Named-Entity-Recognition" class="headerlink" title="Named Entity Recognition"></a>Named Entity Recognition</h4><p>The aim of this task it to predict spans in the text. the result are evaluated comparing the set of spans given in the text with the predicted ones</p>
<h4 id="Relation-Extraction"><a href="#Relation-Extraction" class="headerlink" title="Relation Extraction"></a>Relation Extraction</h4><p>The aim of the relation extraction task is to pre- dict relations and their types between the two enti- ties mentioned in the sentences. The relations with types were compared to annotated data. We use the standard micro-average precision, recall, and F1-score metrics.</p>
<h4 id="Document-multilabel-classification"><a href="#Document-multilabel-classification" class="headerlink" title="Document multilabel classification"></a>Document multilabel classification</h4><p>The idea is to predict multiple labels for a text. It will compare the predicted labels with the tags labels</p>
<h4 id="Inference-Task"><a href="#Inference-Task" class="headerlink" title="Inference Task"></a>Inference Task</h4><p>The aim of this task is to predict if the premise sentence contradicts the hypothesis sentence.</p>
<h2 id="Transfert-Learning-from-supervised-Data"><a href="#Transfert-Learning-from-supervised-Data" class="headerlink" title="Transfert Learning from supervised Data"></a>Transfert Learning from supervised Data</h2><p>In this section we will present all the element which are proper to the solution given. All solution are closed in their implementation however some element may differ from a solution to an other. It is interesting to present all the ele- ment in order to understand all solutions. Understanding those solution may then help find some lead to optimise it a lot more. We define our elements as</p>
<ul>
<li>$\mathcal{T_S}$ : a given source task</li>
<li>$\mathcal{T_T}$ : the targeted task (with $\mathcal{T_T}\neq\mathcal{T_S}$)</li>
</ul>
<p>The solution goal is to improve performances on task $\mathcal{T_T}$</p>
<h3 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h3><p>ULMFit and BERT propose architectures. RoBERTa be- ing an optimisation of BERT algorithm, it is based on BERT classical architecture.<br>It will be interesting to understand how it is working.</p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">Julien Zouein</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://yoursite.com/2020/02/15/transfert-learning-nlp/">http://yoursite.com/2020/02/15/transfert-learning-nlp/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/NLP/">NLP    </a><a class="post-meta__tags" href="/tags/Deep-Learning/">Deep Learning    </a><a class="post-meta__tags" href="/tags/short-paper/">short paper    </a></div><div class="post_share"><div class="social-share" data-image="/img/cover/bert.png" data-sites="facebook,twitter"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js"></script></div></div><nav class="pagination_post" id="pagination"><div class="prev-post pull-full"><a href="/2020/03/30/machine-learning-intro/"><img class="prev_cover lazyload" data-src="/img/cover/ml.jpg" onerror="onerror=null;src='/img/wallpaper_default.jpg'"><div class="label">Previous Post</div><div class="prev_info"><span>Machine Learning Introduction</span></div></a></div></nav></div></main><footer id="footer" data-type="color"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2020 By Julien Zouein</div><div class="framework-info"><span>Driven </span><a href="http://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme </span><a href="https://github.com/jerryc127/hexo-theme-butterfly" target="_blank" rel="noopener"><span>Butterfly</span></a></div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><i class="fa fa-book" id="readmode" title="Read Mode"></i><i class="fa fa-plus" id="font_plus" title="Increase font size"></i><i class="fa fa-minus" id="font_minus" title="Decrease font size"></i><i class="darkmode fa fa-sun-o" id="darkmode" title="Dark Mode"></i></div><div id="rightside-config-show"><div id="rightside_config" title="Setting"><i class="fa fa-cog" aria-hidden="true"></i></div><i class="fa fa-list-ul close" id="mobile-toc-button" title="Table of Contents" aria-hidden="true"></i><i class="fa fa-arrow-up" id="go-up" title="Back to top" aria-hidden="true"></i></div></section><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@latest/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/lazysizes@latest/lazysizes.min.js" async=""></script></body></html>