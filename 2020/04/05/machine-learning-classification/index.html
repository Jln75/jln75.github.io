<!DOCTYPE html><html lang="en" data-theme="dark"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5"><title>Classification | Julien Zouein</title><meta name="description" content="In this course, we will speak about regression in machine Learning"><meta name="keywords" content="Machine Learning,Course,Introduction,classification"><meta name="author" content="Julien Zouein"><meta name="copyright" content="Julien Zouein"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon.ico"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="https://fonts.googleapis.com" crossorigin><link rel="preconnect" href="//busuanzi.ibruce.info"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Classification"><meta name="twitter:description" content="In this course, we will speak about regression in machine Learning"><meta name="twitter:image" content="http://yoursite.com/img/cover/ml.jpg"><meta property="og:type" content="article"><meta property="og:title" content="Classification"><meta property="og:url" content="http://yoursite.com/2020/04/05/machine-learning-classification/"><meta property="og:site_name" content="Julien Zouein"><meta property="og:description" content="In this course, we will speak about regression in machine Learning"><meta property="og:image" content="http://yoursite.com/img/cover/ml.jpg"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script src="https://cdn.jsdelivr.net/npm/js-cookie/dist/js.cookie.min.js"></script><script>const autoChangeMode = 'false'
var t = Cookies.get("theme");
if (autoChangeMode == '1'){
const isDarkMode = window.matchMedia("(prefers-color-scheme: dark)").matches
const isLightMode = window.matchMedia("(prefers-color-scheme: light)").matches
const isNotSpecified = window.matchMedia("(prefers-color-scheme: no-preference)").matches
const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

if (t === undefined){
  if (isLightMode) activateLightMode()
  else if (isDarkMode) activateDarkMode()
  else if (isNotSpecified || hasNoSupport){
    console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
    now = new Date();
    hour = now.getHours();
    isNight = hour < 6 || hour >= 18
    isNight ? activateDarkMode() : activateLightMode()
}
} else if (t == 'light') activateLightMode()
else activateDarkMode()


} else if (autoChangeMode == '2'){
  now = new Date();
  hour = now.getHours();
  isNight = hour < 6 || hour >= 18
  if(t === undefined) isNight? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode() 
} else {
  if ( t == 'dark' ) activateDarkMode()
  else if ( t == 'light') activateLightMode()
}

function activateDarkMode(){
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null){
    document.querySelector('meta[name="theme-color"]').setAttribute('content','#000')
  }
}
function activateLightMode(){
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null){
  document.querySelector('meta[name="theme-color"]').setAttribute('content','#fff')
  }
}</script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="canonical" href="http://yoursite.com/2020/04/05/machine-learning-classification/"><link rel="next" title="Maxhine Learning Introduction" href="http://yoursite.com/2020/03/30/machine-learning-intro/"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  bookmark: {
    title: 'Snackbar.bookmark.title',
    message_prev: 'Press',
    message_next: 'to bookmark this page'
  },
  runtime_unit: 'days',
  runtime: false,
  copyright: undefined,
  ClickShowText: undefined,
  medium_zoom: false,
  fancybox: true,
  Snackbar: undefined,
  baiduPush: false,
  isHome: false,
  isPost: true
  
}</script><meta name="generator" content="Hexo 4.2.0"></head><body><header> <div id="page-header"><span class="pull_left" id="blog_name"><a class="blog_title" id="site-name" href="/">Julien Zouein</a></span><span class="toggle-menu pull_right close"><a class="site-page"><i class="fa fa-bars fa-fw" aria-hidden="true"></i></a></span><span class="pull_right menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> Post</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> About</span></a></div></div></span></div></header><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="/img/profile.jpg" onerror="onerror=null;src='/img/wallpaper_default.jpg'" alt="avatar"></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">Articles</div><div class="length_num">4</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">Tags</div><div class="length_num">8</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">Categories</div><div class="length_num">4</div></a></div></div></div><hr><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> Post</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> About</span></a></div></div></div><div id="mobile-sidebar-toc"><div class="toc_mobile_headline">Catalog</div><div class="sidebar-toc__content"><ol class="toc_mobile_items"><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#Introduction"><span class="toc_mobile_items-number">1.</span> <span class="toc_mobile_items-text">Introduction</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#K-Nearest-Neighbours"><span class="toc_mobile_items-number">2.</span> <span class="toc_mobile_items-text">K-Nearest Neighbours</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#Use-the-KNN"><span class="toc_mobile_items-number">2.1.</span> <span class="toc_mobile_items-text">Use the KNN</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#Calculate-the-similarity-between-two-datapoints"><span class="toc_mobile_items-number">2.2.</span> <span class="toc_mobile_items-text">Calculate the similarity between two datapoints</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#Find-the-right-value-of-K"><span class="toc_mobile_items-number">2.3.</span> <span class="toc_mobile_items-text">Find the right value of K</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#KNN-for-prediction"><span class="toc_mobile_items-number">2.4.</span> <span class="toc_mobile_items-text">KNN for prediction</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#Evaluation-Metrics-for-Classification"><span class="toc_mobile_items-number">3.</span> <span class="toc_mobile_items-text">Evaluation Metrics for Classification</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#Jaccard-Index"><span class="toc_mobile_items-number">3.1.</span> <span class="toc_mobile_items-text">Jaccard Index</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#Confusion-Matrix"><span class="toc_mobile_items-number">3.2.</span> <span class="toc_mobile_items-text">Confusion Matrix</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#F1-Score"><span class="toc_mobile_items-number">3.3.</span> <span class="toc_mobile_items-text">F1 Score</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#Log-Loss"><span class="toc_mobile_items-number">3.4.</span> <span class="toc_mobile_items-text">Log Loss</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#Decision-Tree"><span class="toc_mobile_items-number">4.</span> <span class="toc_mobile_items-text">Decision Tree</span></a></li></ol></div></div></div><div id="body-wrap"><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true">     </i><div id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Introduction"><span class="toc-number">1.</span> <span class="toc-text">Introduction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#K-Nearest-Neighbours"><span class="toc-number">2.</span> <span class="toc-text">K-Nearest Neighbours</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Use-the-KNN"><span class="toc-number">2.1.</span> <span class="toc-text">Use the KNN</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Calculate-the-similarity-between-two-datapoints"><span class="toc-number">2.2.</span> <span class="toc-text">Calculate the similarity between two datapoints</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Find-the-right-value-of-K"><span class="toc-number">2.3.</span> <span class="toc-text">Find the right value of K</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#KNN-for-prediction"><span class="toc-number">2.4.</span> <span class="toc-text">KNN for prediction</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Evaluation-Metrics-for-Classification"><span class="toc-number">3.</span> <span class="toc-text">Evaluation Metrics for Classification</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Jaccard-Index"><span class="toc-number">3.1.</span> <span class="toc-text">Jaccard Index</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Confusion-Matrix"><span class="toc-number">3.2.</span> <span class="toc-text">Confusion Matrix</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#F1-Score"><span class="toc-number">3.3.</span> <span class="toc-text">F1 Score</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Log-Loss"><span class="toc-number">3.4.</span> <span class="toc-text">Log Loss</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Decision-Tree"><span class="toc-number">4.</span> <span class="toc-text">Decision Tree</span></a></li></ol></div></div></div><main id="content-outer"><div id="top-container" style="background-image: url(/img/cover/ml.jpg)"><div id="post-info"><div id="post-title"><div class="posttitle">Classification</div></div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> Created 2020-04-05<span class="post-meta__separator">|</span><i class="fa fa-history fa-fw" aria-hidden="true"></i> Updated 2020-05-02</time><span class="post-meta__separator">|</span><span><i class="fa fa-inbox post-meta__icon fa-fw" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/Course/">Course</a><i class="fa fa-angle-right fa-fw" aria-hidden="true"></i><i class="fa fa-inbox post-meta__icon fa-fw" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/Course/Machine-Learning/">Machine Learning</a></span><div class="post-meta-wordcount"><div class="post-meta-pv-cv"><span><i class="fa fa-eye post-meta__icon fa-fw" aria-hidden="true"> </i>Post View:</span><span id="busuanzi_value_page_pv"></span></div></div></div></div></div><div class="layout layout_post" id="content-inner">   <article id="post"><div class="article-container" id="post-content"><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>In Machine learning, Classification is a supervised method used to find a discrete value associated to a data.$\$<br>An exemple of classification is to say if a cell is malignant or benign.</p>
<p>Our goal is to find the relation between a set of features and a taget (which is the class we want to find).</p>
<hr>
<p>We want to know if a cell is malignant or benign. To do so we will use historical data on malignant and benign cell and analyze all their featrures.</p>
<p>We may have for exemple, the size of the cell, the aera of the cell, perimeter of the cell, and in our historical data, we will also have the class of each cell (if it is benign or not).</p>
<p>By training on this data, we want to be able to predict if a unknown cell is benign or not, based on its own features.</p>
<hr>
<p>In the given exemple, we have a binary classification, however, we can have multiple-class classifier.</p>
<p>There are a lot of classification algorithm, we will present fiew of them in this chapter.</p>
<h2 id="K-Nearest-Neighbours"><a href="#K-Nearest-Neighbours" class="headerlink" title="K-Nearest Neighbours"></a>K-Nearest Neighbours</h2><p>The first method we will see is the K-Nearest Neighbours (or KNN)$\$</p>
<hr>
<p>To have a better comprehension of KNN, image you want to predict the class of a cell (Benign or Malignant). We will use a Scatter plot to show the repartitions according to two features.</p>
<p>Given this repartition of point, what happens if we had a new point ? How to determine the class of this point ?</p>
<p>The K-Nearest Neighbours take the K neigbours of our new point to determine the class of our point.</p>
<p>In the case of 1-NN, our new point will have the same class as the nearest neigbours on the graph.</p>
<hr>
<p>KNN is an algorithm based on similarity. A new case will be classified the same way, similar cases are classified. It takes a batch of labeled point to determine the label of a new one.</p>
<h3 id="Use-the-KNN"><a href="#Use-the-KNN" class="headerlink" title="Use the KNN"></a>Use the KNN</h3><ol>
<li>Pick a Value for K, finding the right K will be one of the important problem to solve</li>
<li>Calculate the distances between the unlabeled points and the K nearest neighbours</li>
<li>Predict the value for each new cases, based on the most popular response given by the KNN</li>
</ol>
<h3 id="Calculate-the-similarity-between-two-datapoints"><a href="#Calculate-the-similarity-between-two-datapoints" class="headerlink" title="Calculate the similarity between two datapoints"></a>Calculate the similarity between two datapoints</h3><p>In a case where we have dartapoints with only one feature. It is easy to use an Euclidean Distance to calculate the similarity between the two features :<br>$$ Dis(x_{1},x_{2})=\sqrt{(x_{1}-x_{2})^{2}}$$</p>
<p>We can still use this method with more then one feature. The formula will just change a bit :<br>$$Dis(c_{1},c_{2})=\sqrt{\sum_{i=1}^{n}(c_{1i}-c_{2i})^{2}}$$<br>It is important to normalize our data using this method. Indeed, image we take two features (Age and Income), the value of the income will always be greater then the value of the age. Using Euclidean distance without normalizing the feature will give a higher weight to income and a lower to age.</p>
<h3 id="Find-the-right-value-of-K"><a href="#Find-the-right-value-of-K" class="headerlink" title="Find the right value of K"></a>Find the right value of K</h3><p>By changing the value of K, we may change the prediction for a case. That is why it is important to find the good value of K.</p>
<p>By taking K=1, we create a model with a high probability of overfitting. That means it will not generalize to unknown data.</p>
<p>By taking a too high value for K, we will catch neighbours that may not be related to our point and will probably have an underfitting algorithm.</p>
<p>A method to find K is to keep a part of the data to do some testing. We will fit our algorithm with different values of K and then compute the accuracy on our test set. The algorithm with the best accuracy shall give us the best value of K.</p>
<h3 id="KNN-for-prediction"><a href="#KNN-for-prediction" class="headerlink" title="KNN for prediction"></a>KNN for prediction</h3><p>KNN can also be used as an algorithm for regression. Instead of choosing the most represented label in the neighbours, we will take the mean of all the neighbours for exemple and it will give us the continuous value we were looking for.</p>
<hr>
<p>Imagine we use KNN for house price prediction. We will find the house with the most similarity regarding to our selected features. Once we will choose our K, we will compute the mean of the price of the K Nearest Neighbours and it may give us an estimation of the price of our house</p>
<hr>
<h2 id="Evaluation-Metrics-for-Classification"><a href="#Evaluation-Metrics-for-Classification" class="headerlink" title="Evaluation Metrics for Classification"></a>Evaluation Metrics for Classification</h2><p>Before seing other Classification Methods, is is useful to know some metrics used for Classification.</p>
<h3 id="Jaccard-Index"><a href="#Jaccard-Index" class="headerlink" title="Jaccard Index"></a>Jaccard Index</h3><p>It is one of the simplest accuracy measure.<br>We have :<br>$$J(\hat{y},y)=\frac{|y\cap\hat{y}|}{|y\cup\hat{y}|}=\frac{|y\cap\hat{y}|}{|y|+|\hat{y}|-|y\cap\hat{y}|}$$<br>This index is used in Convolutionnal Network and is know as Intersection over Union (IoU)</p>
<h3 id="Confusion-Matrix"><a href="#Confusion-Matrix" class="headerlink" title="Confusion Matrix"></a>Confusion Matrix</h3><p>An other way to calculate the accuracy is to look for the confusion Matrix.<br>This Matrix give us the correct and wrong predictions. Each row give us a real class in the test set, and each columns give us the predicted values.</p>
<hr>
<p>For exemple, the first row will be all the “Benign” cells and the second row all the “Malignant” one.<br>The first column will be all the cells classified by the algorithm as “Benign” and the second column all the cells classified by the algorithm as “Malignant”.</p>
<hr>
<p>In a more general way, a confusion Matrix is as shown :<br>| |Class 1 | Class 2 |<br>|—|—|—|<br>Class 1 | True Positive | False Negative|<br>Class 2 | False Positive | True Negative|</p>
<p>All those elements can be used to calcultate some metrics :</p>
<p>$\begin{array}{lll}<br>  \text{precision}&amp;=&amp; \frac{TP}{(TP+FP)}\<br>  \text{recall}&amp;=&amp;\frac{TP}{(TP+FN)}<br>\end{array}$</p>
<ul>
<li>precision : This measure give us the number of label well classiified. In the case of cells classification, it represents the number of cells that are well classified as Malignant cells</li>
<li>recall : This measure give us the number of cases classified as true and which are really true. In the case of cell classification, it will give us the number of cells predicted as malignant among all the malignant cells</li>
</ul>
<p>It is important to see that a higher recalls means a lower precision and a higer precision means a lower recalls. In that case, how to know which one to maximize ?</p>
<p>The solution is to calculate an other metrics based on this one called the F1 Score</p>
<h3 id="F1-Score"><a href="#F1-Score" class="headerlink" title="F1 Score"></a>F1 Score</h3><p>The F1 score is the harmonic average of the precision and recall, the formula is :<br>$$F1-score = 2\times\frac{\text{precision}\times\text{recall}}{\text{precision}+\text{recall}}$$<br>We want to maximize this F1-score (maximum value being 1). It is a good way to be sure that our classifier as good values for recall and precision.</p>
<h3 id="Log-Loss"><a href="#Log-Loss" class="headerlink" title="Log Loss"></a>Log Loss</h3><p>In the previous exemple is KNN, we have seen that our output was our class label. However, we can have as an output, the probability for a class label. In that case we need an other metric to calculate the accuracy.</p>
<p>The log loss equations is :<br>$$loss=(y\times\log(\hat{y})+(1-y)\times\log(1-\hat{y}))$$<br>It tells us, how far a prediction is from actual label. A bad prediction will result in a high loss log score.<br>Based on the log loss, we calculate the average log loss to know our model accuracy :<br>$$Logloss = -\frac{1}{n}\sum_{i=1}^{n}(y\times\log(\hat{y})+(1-y)\times\log(1-\hat{y}))$$<br>To maximize our accuracy we need to minimise the LogLoss value. When it is near 0, our model have a good accuracy, however, when it is close to one, we have a bad accuracy.</p>
<h2 id="Decision-Tree"><a href="#Decision-Tree" class="headerlink" title="Decision Tree"></a>Decision Tree</h2></div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">Julien Zouein</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://yoursite.com/2020/04/05/machine-learning-classification/">http://yoursite.com/2020/04/05/machine-learning-classification/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Machine-Learning/">Machine Learning    </a><a class="post-meta__tags" href="/tags/Course/">Course    </a><a class="post-meta__tags" href="/tags/Introduction/">Introduction    </a><a class="post-meta__tags" href="/tags/classification/">classification    </a></div><div class="post_share"><div class="social-share" data-image="/img/cover/ml.jpg" data-sites="facebook,twitter"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js"></script></div></div><nav class="pagination_post" id="pagination"><div class="next-post pull-full"><a href="/2020/03/30/machine-learning-intro/"><img class="next_cover lazyload" data-src="/img/cover/ml.jpg" onerror="onerror=null;src='/img/wallpaper_default.jpg'"><div class="label">Next Post</div><div class="next_info"><span>Maxhine Learning Introduction</span></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fa fa-fw fa-thumbs-up" aria-hidden="true"></i><span> Recommend</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2020/03/30/machine-learning-intro/" title="Maxhine Learning Introduction"><img class="relatedPosts_cover lazyload"data-src="/img/cover/ml.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-03-30</div><div class="relatedPosts_title">Maxhine Learning Introduction</div></div></a></div><div class="relatedPosts_item"><a href="/2020/03/30/machine-learning-regression/" title="Regression"><img class="relatedPosts_cover lazyload"data-src="/img/cover/ml.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-03-30</div><div class="relatedPosts_title">Regression</div></div></a></div></div><div class="clear_both"></div></div></div></main><footer id="footer" data-type="color"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2020 By Julien Zouein</div><div class="framework-info"><span>Driven </span><a href="http://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme </span><a href="https://github.com/jerryc127/hexo-theme-butterfly" target="_blank" rel="noopener"><span>Butterfly</span></a></div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><i class="fa fa-book" id="readmode" title="Read Mode"></i><i class="fa fa-plus" id="font_plus" title="Increase font size"></i><i class="fa fa-minus" id="font_minus" title="Decrease font size"></i><i class="darkmode fa fa-sun-o" id="darkmode" title="Dark Mode"></i></div><div id="rightside-config-show"><div id="rightside_config" title="Setting"><i class="fa fa-cog" aria-hidden="true"></i></div><i class="fa fa-list-ul close" id="mobile-toc-button" title="Table of Contents" aria-hidden="true"></i><i class="fa fa-arrow-up" id="go-up" title="Back to top" aria-hidden="true"></i></div></section><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@latest/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/lazysizes@latest/lazysizes.min.js" async=""></script></body></html>